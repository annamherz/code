{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import BioSimSpace as BSS\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import itertools\n",
    "\n",
    "print(\"adding code to the pythonpath...\")\n",
    "code = '/home/anna/Documents/code/python'\n",
    "if code not in sys.path:\n",
    "    sys.path.insert(1, code)\n",
    "import pipeline\n",
    "\n",
    "from pipeline.prep import *\n",
    "from pipeline.utils import *\n",
    "\n",
    "pipeline.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now want to start putting it all together by initialising the pipeline\n",
    "# this is so can have all the file locations \n",
    "\n",
    "pl = initialise_pipeline()\n",
    "# where the ligands for the pipeline are located. These should all be in the same folder in sdf format\n",
    "pl.ligands_folder(f\"/home/anna/Documents/benchmark/inputs/tyk2/ligands\")\n",
    "# where the pipeline should be made\n",
    "pl.main_folder(\"/home/anna/Documents/code/write/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make default protocols\n",
    "pl.setup_protocols()\n",
    "\n",
    "# edit as needed\n",
    "\n",
    "#setup ligands and network\n",
    "pl.setup_ligands()\n",
    "pl.setup_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:teal\">Protein parameterisation</span>  \n",
    "\n",
    "This needs to be carried out carefully.\n",
    "\n",
    "can parameterise using :\n",
    "```python\n",
    "prot = BSS.IO.readPDB(path_to_protein, pdb4amber=False)[0]\n",
    "prot_p = BSS.Parameters.parameterise(prot, protocol.protein_forcefield()).getMolecule()\n",
    "BSS.IO.saveMolecules(\"inputs/protein\", prot_p, [\"PRM7\",\"RST7\"])\n",
    "```\n",
    "\n",
    "tleap may fail. Best to parameterise carefully before and also consider crystal waters.\n",
    "\n",
    "can view using:\n",
    "```python\n",
    "BSS.Notebook.View(f\"{input_dir}/{protein}/protein/{protein}_parameterised.pdb\").system()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the protein file locations to the pipeline setup object\n",
    "pl.protein_path(f\"/home/anna/Documents/benchmark/inputs/tyk2/tyk2_parameterised\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the run_all script, also does a final ligand and network write\n",
    "pl.write_run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:teal\">Generating the RBFENN</span>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first want to make all the stuff for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sem perturbations\n",
    "tgt_to_run = f\"{protein}_rename\" #f\"{protein}_me\" f\"{protein}_rename\" for tyk2 and p38\n",
    "cats_files_path = f\"{main_folder}/scripts/RBFENN/ANALYSIS/perturbation_networks/output/series_predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleArray(arr):\n",
    "    \"\"\"Scales an array to be the inverse in the range [0-1].\"\"\"\n",
    "    \n",
    "    # normalise to the range 0-1.\n",
    "    return minmax_scale(1 /  arr, feature_range=(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the FEPNN SEM prediction per ligand.\n",
    "perts = {}\n",
    "for cats_file in glob.glob(f\"{cats_files_path}/{tgt_to_run}_*\"):\n",
    "    \n",
    "    with open(cats_file, \"r\") as readfile:\n",
    "        reader = csv.reader(readfile)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            pert = row[0]\n",
    "            pred_sem = float(row[1])\n",
    "            \n",
    "            if not pert in perts:\n",
    "                perts[pert] = [pred_sem]\n",
    "            else:\n",
    "                perts[pert].append(pred_sem)\n",
    "            \n",
    "# compute the mean SEM prediction per pert.\n",
    "pert_names = []\n",
    "pert_sems = []\n",
    "for pert, sems in perts.items():\n",
    "    mean_sem = np.mean(sems)\n",
    "    pert_names.append(pert)\n",
    "    pert_sems.append(float(mean_sem))\n",
    "\n",
    "#Â now scale the sems to [0-1].\n",
    "pert_sems = scaleArray(np.array(pert_sems))\n",
    "\n",
    "for pert, val in zip(pert_names, pert_sems):\n",
    "    perts[pert] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make folder for the RBFENN network\n",
    "validate.folder_path(f\"{pl.exec_folder()}/RBFENN\", create=True)\n",
    "\n",
    "written = []\n",
    "with open(f\"{pl.exec_folder()}/RBFENN/links_file.in\", \"w\") as writefile:\n",
    "    writer = csv.writer(writefile, delimiter =\" \")\n",
    "    \n",
    "    for pert_name, value in perts.items():\n",
    "        # find the lomap filename for both ligs.\n",
    "        liga_lomap_name = None\n",
    "        ligb_lomap_name = None\n",
    "        for filename in glob.glob(f\"{pl.ligands_folder()}/*.sdf\"):\n",
    "            # if \"lig_8\" in filename:\n",
    "            #     continue # exclude +1 ligands from tnks2 set.\n",
    "            if pert_name.split(\"~\")[0] in filename:\n",
    "                liga_lomap_name = filename.split(\"/\")[-1].split(\".\")[0]#.replace(\"ejm\",\"ejm_\").replace(\"jmc\",\"jmc_\")\n",
    "            elif pert_name.split(\"~\")[1] in filename:\n",
    "                ligb_lomap_name = filename.split(\"/\")[-1].split(\".\")[0]#.replace(\"ejm\",\"ejm_\").replace(\"jmc\",\"jmc_\")\n",
    "            \n",
    "            if liga_lomap_name and ligb_lomap_name:\n",
    "                if not [liga_lomap_name, ligb_lomap_name] in written:\n",
    "                    writer.writerow([liga_lomap_name, ligb_lomap_name, value])\n",
    "                    \n",
    "                    written.append([liga_lomap_name, ligb_lomap_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ligands and ligands_names already exists due to lomap above\n",
    "# if change the folder name, will put this in the execution model as default.\n",
    "pl.setup_network(folder=\"RBFENN\", links_file=f\"{pl.exec_folder()}/RBFENN/links_file.in\")\n",
    "\n",
    "# this will update the existing network.\n",
    "pl.write_network(file_path=f\"{pl.exec_folder()}/rbfenn_network.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the rbfenn to a different network file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:teal\">Comparing lomap and the rbfenn</span>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the perts in each and all together\n",
    "perts_lomap = []\n",
    "perts_rbfenn = []\n",
    "perts = []\n",
    "\n",
    "with open(f\"{exec_folder}/network_rbfenn_scores.dat\", \"r\") as fepnn_file, \\\n",
    "        open(f\"{exec_folder}/network_lomap_scores.dat\", \"r\") as lomap_file:\n",
    "    reader_fepnn = csv.reader(fepnn_file)\n",
    "    reader_lomap = csv.reader(lomap_file)\n",
    "    \n",
    "    for line in reader_fepnn:\n",
    "        perts_rbfenn.append(f\"{line[0]}~{line[1]}\")\n",
    "        perts.append(f\"{line[0]}~{line[1]}\")\n",
    "    for line in reader_lomap:\n",
    "        perts_lomap.append(f\"{line[0]}~{line[1]}\")\n",
    "        perts.append(f\"{line[0]}~{line[1]}\")\n",
    "\n",
    "# write a file that contains the combined perts, directions are distinct     \n",
    "combined_perts = []\n",
    "filtered_out = 0\n",
    "for pert in perts:\n",
    "    \n",
    "    if not pert in combined_perts:\n",
    "        combined_perts.append(pert)\n",
    "    else:\n",
    "        filtered_out += 1\n",
    "print(f\"Removed {filtered_out} duplicate perts between lomap and rbfenn to give {len(combined_perts)} combined perts.\")\n",
    "\n",
    "# write a file that contains the unique perts, 1 direction only.      \n",
    "filtered_perts = []\n",
    "filtered_out = 0\n",
    "for pert in combined_perts:\n",
    "    \n",
    "    inv_pert = pert.split(\"~\")[1]+\"~\"+pert.split(\"~\")[0]\n",
    "    \n",
    "    if not pert in filtered_perts and not inv_pert in filtered_perts:\n",
    "        filtered_perts.append(pert)\n",
    "    else:\n",
    "        filtered_out += 1\n",
    "print(f\"Removed {filtered_out} inverse perts to give {len(filtered_perts)} unique perts, one direction only.\")\n",
    "\n",
    "# get the perts that are unique to each\n",
    "unique_perts = []\n",
    "unique_out_lomap = 0\n",
    "unique_out_rbfenn = 0\n",
    "shared_out = 0\n",
    "\n",
    "for pert in perts_lomap:\n",
    "    \n",
    "    inv_pert = pert.split(\"~\")[1]+\"~\"+pert.split(\"~\")[0]\n",
    "    \n",
    "    if not pert in perts_rbfenn and not inv_pert in perts_rbfenn:\n",
    "        unique_perts.append((pert, \"lomap\"))\n",
    "        unique_out_lomap += 1\n",
    "\n",
    "for pert in perts_rbfenn:\n",
    "    \n",
    "    inv_pert = pert.split(\"~\")[1]+\"~\"+pert.split(\"~\")[0]\n",
    "    \n",
    "    if not pert in perts_lomap and not inv_pert in perts_lomap:\n",
    "        unique_perts.append((pert, \"rbfenn\"))\n",
    "        unique_out_rbfenn += 1\n",
    "    \n",
    "for pert in combined_perts:\n",
    "\n",
    "    inv_pert = pert.split(\"~\")[1]+\"~\"+pert.split(\"~\")[0]\n",
    "\n",
    "    if pert in perts_lomap or inv_pert in perts_lomap:\n",
    "        if pert in perts_rbfenn or inv_pert in perts_rbfenn:\n",
    "            unique_perts.append((pert, \"shared\"))\n",
    "            shared_out += 1\n",
    "\n",
    "       \n",
    "print(f\"There are {unique_out_lomap} pert(s) unique to lomap and {unique_out_rbfenn} pert(s) unique to rbfenn.\")\n",
    "print(f\"There are {shared_out} pert(s) shared between lomap and rbfenn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{exec_folder}/combined_perts.dat\", \"w\") as writefile:\n",
    "    writer = csv.writer(writefile)\n",
    "    for pert in combined_perts:\n",
    "        writer.writerow([pert])\n",
    "print(f\"Total number of combined perturbations: {len(combined_perts)}\")\n",
    "\n",
    "with open(f\"{exec_folder}/filtered_perts.dat\", \"w\") as writefile:\n",
    "    writer = csv.writer(writefile)\n",
    "    for pert in filtered_perts:\n",
    "        writer.writerow([pert])\n",
    "print(f\"Total number of filtered perturbations: {len(filtered_perts)}\")\n",
    "\n",
    "# write a file for the different perts\n",
    "with open(f\"{exec_folder}/unique_perts.dat\", \"w\") as writefile:\n",
    "    writer = csv.writer(writefile)\n",
    "    for pert in unique_perts:\n",
    "        writer.writerow([pert[0],pert[1]])\n",
    "print(f\"Total number of unique perturbations: {len(unique_perts)} (lomap: {unique_out_lomap}, rbfenn: {unique_out_rbfenn})\")\n",
    "print(f\"Total number of shared perturbations: {shared_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dicitonary of the perts for plotting the nx graph\n",
    "both_pert_networks_dict = {}\n",
    "\n",
    "for pert in filtered_perts:\n",
    "\n",
    "    inv_pert = pert.split(\"~\")[1]+\"~\"+pert.split(\"~\")[0]\n",
    "    \n",
    "    if pert in perts_lomap and pert in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"both\"\n",
    "    elif inv_pert in perts_lomap and pert in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"both\"\n",
    "    elif pert in perts_lomap and pert not in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"lomap\"\n",
    "    elif inv_pert in perts_lomap and pert not in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"lomap\"\n",
    "    elif pert not in perts_lomap and pert in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"rbfenn\"\n",
    "    elif inv_pert not in perts_lomap and pert in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"rbfenn\"      \n",
    "\n",
    "# create dict for images for the nx graph\n",
    "image_dict = {}\n",
    "# list files in inputs\n",
    "input_files_for_image = sorted(os.listdir(f\"{exec_folder}/visualise_network_lomap/inputs\"))\n",
    "for in_file in input_files_for_image:\n",
    "    lig_name_list = in_file.split(\"_\")[1:]\n",
    "    lig_name = '_'.join(lig_name_list).split(\".\")[0]\n",
    "    lig_number = in_file.split(\"_\")[0]\n",
    "    image_dict[lig_name] = lig_number   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_pert_networks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph.\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Loop over the nligands and add as nodes to the graph.\n",
    "for lig in ligand_names:\n",
    "    img = f\"{exec_folder}/visualise_network_lomap/images/{image_dict[lig]}.png\"\n",
    "    graph.add_node(lig, image=img, label=lig, labelloc=\"t\")\n",
    "\n",
    "# make a dict of colours\n",
    "# navy teal  #CC00CC\n",
    "# clear is '#FF000000' \n",
    "colour_dict = {\"both\":'navy' ,\"lomap\":'teal' ,\"rbfenn\":'hotpink' }\n",
    "\n",
    "# Loop over the edges in the dictionary and add to the graph.\n",
    "for edge in both_pert_networks_dict:\n",
    "    graph.add_edge(edge.split(\"~\")[0],edge.split(\"~\")[1],\n",
    "                    color=colour_dict[both_pert_networks_dict[edge]]\n",
    "                    )\n",
    "\n",
    "# Plot the networkX graph.\n",
    "pos = nx.kamada_kawai_layout(graph)\n",
    "colours = nx.get_edge_attributes(graph,'color').values()\n",
    "\n",
    "plt.figure(figsize=(12,12), dpi=150)\n",
    "nx.draw(\n",
    "    graph, pos, edge_color=colours, width=1, linewidths=5,\n",
    "    node_size=2000, node_color='skyblue', font_size = 12,\n",
    "    labels={node: node for node in graph.nodes()})\n",
    "\n",
    "plt.savefig(f\"{exec_folder}/compared_networks_no_images.png\", dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# Convert to a dot graph.\n",
    "dot_graph = nx.drawing.nx_pydot.to_pydot(graph)\n",
    "\n",
    "# Write to a PNG.\n",
    "network_plot = f\"{exec_folder}/compared_networks.png\"\n",
    "dot_graph.write_png(network_plot)\n",
    "\n",
    "# Create a plot of the network.\n",
    "img = mpimg.imread(network_plot)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the lomap score for the combined network file (unfiltered)\n",
    "combined_pert_network_dict = {}\n",
    "\n",
    "for pert in combined_perts:\n",
    "    lig_a = pert.split(\"~\")[0]\n",
    "    lig_b = pert.split(\"~\")[1]\n",
    "    # then, we need to find this index for our chosen edge that we are adding.\n",
    "    lig_a_index = ligand_names.index(lig_a)\n",
    "    lig_b_index = ligand_names.index(lig_b)\n",
    "    # finally, we need to calculate this single lomap score.\n",
    "    single_transformation, single_lomap_score = BSS.Align.generateNetwork([ligands[lig_a_index], ligands[lig_b_index]], names=[ligand_names[lig_a_index], ligand_names[lig_b_index]], plot_network=False)\n",
    "    print(f\"LOMAP score for {lig_a} to {lig_b} is {single_lomap_score[0]} .\")\n",
    "    combined_pert_network_dict[(lig_a, lig_b)] = single_lomap_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the combined to a different network file\n",
    "\n",
    "with open(f\"{exec_folder}/network_combined.dat\", \"w\") as network_file:\n",
    "\n",
    "    writer = csv.writer(network_file, delimiter=\" \")\n",
    "    \n",
    "    for pert, score in combined_pert_network_dict.items():\n",
    "        # # based on the provided (at top of notebook) lambda allocations and LOMAP threshold, decide allocation.\n",
    "        # if score == None or score < float(node.getInput(\"LOMAP Threshold\")):\n",
    "        #     num_lambda = node.getInput(\"DiffLambdaWindows\")\n",
    "        # else:\n",
    "        #     num_lambda = node.getInput(\"LambdaWindows\")\n",
    "        \n",
    "        num_lambda = node.getInput(\"LambdaWindows\")            \n",
    "       \n",
    "        # given the number of allocated lambda windows, generate an array for parsing downstream.\n",
    "        lam_array_np = np.around(np.linspace(0, 1, int(num_lambda)), decimals=5)\n",
    "\n",
    "        # make the array into a format readable by bash.\n",
    "        lam_array = str(lam_array_np).replace(\"[ \", \"\").replace(\"]\", \"\").replace(\"  \", \",\").replace('\\n', '')\n",
    "\n",
    "        # write out both directions for this perturbation.\n",
    "        if engine == \"ALL\":\n",
    "            for eng in BSS.FreeEnergy.engines():\n",
    "                writer.writerow([pert[0], pert[1], len(lam_array_np), lam_array, eng])\n",
    "        else:\n",
    "            writer.writerow([pert[0], pert[1], len(lam_array_np), lam_array, engine])\n",
    "        # writer.writerow([pert[1], pert[0], len(lam_array_np), lam_array, engine])         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biosimspace-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d79bb85316fa6c998e385cc39903e056bffeb3f6098416e9c269ddd32175e919"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
