{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "from scipy.stats import sem as sem\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "if \"/home/anna/Documents/cinnabar\" not in sys.path:\n",
    "    sys.path.insert(1, \"/home/anna/Documents/cinnabar\")\n",
    "import cinnabar\n",
    "\n",
    "print(\"adding code to the pythonpath...\")\n",
    "code = \"/home/anna/Documents/code/python\"\n",
    "if code not in sys.path:\n",
    "    sys.path.insert(1, code)\n",
    "import pipeline\n",
    "\n",
    "print(cinnabar.__file__)\n",
    "\n",
    "from pipeline import *\n",
    "from pipeline.utils import validate\n",
    "from pipeline.analysis import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_folder = f\"/home/anna/Documents/benchmark\"\n",
    "protein = \"tyk2\"\n",
    "main_dir = f\"{bench_folder}/extracted/{protein}\"\n",
    "\n",
    "# choose location for the files\n",
    "net_file = f\"{main_dir}/execution_model/network_combined.dat\"\n",
    "ana_file = f\"{main_dir}/execution_model/analysis_protocol.dat\"\n",
    "exp_file = f\"{bench_folder}/inputs/experimental/{protein}.yml\"\n",
    "\n",
    "if os.path.exists(f\"{main_dir}/outputs_extracted/results\"):\n",
    "    results_folder = f\"{main_dir}/outputs_extracted/results\"\n",
    "elif os.path.exists(f\"{main_dir}/outputs/results\"):\n",
    "    results_folder = f\"{main_dir}/outputs/results\"\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"results directory not found in the {main_dir}. please make sure results were written using the analysis script previously in the pipeline\"\n",
    "    )\n",
    "\n",
    "output_folder = validate.folder_path(f\"{main_dir}/analysis\", create=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_analysis_object = analysis_network(\n",
    "    results_folder,\n",
    "    exp_file=exp_file,\n",
    "    net_file=net_file,\n",
    "    output_folder=output_folder,\n",
    "    analysis_ext=ana_file,\n",
    ")\n",
    "\n",
    "# can add any other results files\n",
    "# all_analysis_object.compute_other_results(file_name=None, name=None)\n",
    "all_analysis_object.compute(cycle_closure=False, use_cinnabar=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is uncertainty for a single run the same across triplicates?\n",
    "does it agree with uncertainty across triplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error for a perturbation for triplicate\n",
    "\n",
    "uncertainty_dict = {}\n",
    "\n",
    "for eng in all_analysis_object.engines:\n",
    "    uncertainty_dict[eng] = {}\n",
    "    print(f\"{eng}\")\n",
    "\n",
    "    for pert in all_analysis_object.calc_pert_dict[eng]:\n",
    "        uncertainty_dict[eng][pert] = all_analysis_object.calc_pert_dict[eng][pert][1]\n",
    "\n",
    "# uncertainty_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error for a perturbation per single run\n",
    "\n",
    "uncertainty_dict_single = {}\n",
    "\n",
    "for eng in all_analysis_object.engines:\n",
    "    uncertainty_dict_single[eng] = {}\n",
    "    repeat = 0\n",
    "    for file in all_analysis_object._results_repeat_files[eng]:\n",
    "        uncertainty_dict_single[eng][repeat] = {}\n",
    "        calc_diff_dict = make_dict.comp_results(\n",
    "            file, all_analysis_object.perturbations, eng, name=None\n",
    "        )\n",
    "\n",
    "        for pert in calc_diff_dict.keys():\n",
    "            uncertainty_dict_single[eng][repeat][pert] = calc_diff_dict[pert][1]\n",
    "\n",
    "        repeat += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make list of single run uncertainties\n",
    "can feed into histogram function and also average to compare\n",
    "\n",
    "abstract histogram plotting so can put in data\n",
    "abstract other plotting functions so can put in whatever\n",
    "also need to abstract colour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "all_analysis_object.plot_histogram_sem(pert_val=\"pert\")\n",
    "\n",
    "all_analysis_object.plot_histogram_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the triplicate result to the single result compared to experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncertainty for triplicates compared to experimental\n",
    "\n",
    "for eng in all_analysis_object.engines:\n",
    "    eng_mue = all_analysis_object._stats_object.compute_mue(\"pert\", y=eng)\n",
    "    print(f\"MUE for {eng} is {eng_mue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncetainty for single runs compared to experimental\n",
    "\n",
    "for eng in all_analysis_object.engines:\n",
    "    repeat = 0\n",
    "\n",
    "    for file in all_analysis_object._results_repeat_files[eng]:\n",
    "        calc_diff_dict = make_dict.comp_results(\n",
    "            file, all_analysis_object.perturbations, eng, name=None\n",
    "        )\n",
    "\n",
    "        df = plotting_engines.match_dicts_to_df(\n",
    "            all_analysis_object.exper_pert_dict, calc_diff_dict, \"experimental\", \"calc\"\n",
    "        )\n",
    "\n",
    "        df = df.dropna()\n",
    "\n",
    "        x = df[f\"freenrg_experimental\"]\n",
    "        y = df[f\"freenrg_calc\"]\n",
    "        xerr = df[f\"err_experimental\"]\n",
    "        yerr = df[f\"err_calc\"]\n",
    "\n",
    "        res = stats_engines.compute_stats(x, y, xerr, yerr, \"MUE\")\n",
    "\n",
    "        print(f\"MUE for {eng} repeat {repeat} is {res}\")\n",
    "        repeat += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
