{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/anna/anaconda3/envs/biosimspace-dev/bin/pmemd.cuda'\n",
      "could not determine AMBER version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning on use of the timeseries module: If the inherent timescales of the system are long compared to those being analyzed, this statistical inefficiency may be an underestimate.  The estimate presumes the use of many statistically independent samples.  Tests should be performed to assess whether this condition is satisfied.   Be cautious in the interpretation of the data.\n",
      "/home/anna/anaconda3/envs/biosimspace-dev/lib/python3.9/site-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/anna/anaconda3/envs/biosimspace-dev/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/anna/anaconda3/envs/biosimspace-dev/lib/python3.9/site-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/anna/anaconda3/envs/biosimspace-dev/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================================\n",
      "Sending anonymous Sire usage statistics to http://siremol.org.\n",
      "For more information, see http://siremol.org/analytics\n",
      "To disable, set the environment variable 'SIRE_DONT_PHONEHOME' to 1\n",
      "To see the information sent, set the environment variable \n",
      "SIRE_VERBOSE_PHONEHOME equal to 1. To silence this message, set\n",
      "the environment variable SIRE_SILENT_PHONEHOME to 1.\n",
      "==============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: importing 'simtk.openmm' is deprecated.  Import 'openmm' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding code to the pythonpath...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/anaconda3/envs/biosimspace-dev/lib/python3.9/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import BioSimSpace as BSS\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale, MinMaxScaler\n",
    "import itertools as it\n",
    "\n",
    "\n",
    "print(\"adding code to the pythonpath...\")\n",
    "code = \"/home/anna/Documents/code/python\"\n",
    "if code not in sys.path:\n",
    "    sys.path.insert(1, code)\n",
    "import pipeline\n",
    "\n",
    "from pipeline.prep import *\n",
    "from pipeline.utils import *\n",
    "from pipeline.setup import initialise_pipeline\n",
    "\n",
    "pipeline.__file__\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "fwf_path = (\n",
    "    \"/home/anna/Documents/september_2022_workshops/freenrgworkflows/networkanalysis\"\n",
    ")\n",
    "if fwf_path not in sys.path:\n",
    "    sys.path.insert(1, fwf_path)\n",
    "\n",
    "import networkanalysis\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from scipy.stats import sem as sem\n",
    "from scipy.stats import bootstrap, norm\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = \"tyk2\"\n",
    "\n",
    "exec_folder = f\"/home/anna/Documents/benchmark/{protein}_benchmark/execution_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:teal\">Comparing lomap and the rbfenn</span>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the perts in each and all together\n",
    "perts_lomap = []\n",
    "perts_rbfenn = []\n",
    "perts = []\n",
    "\n",
    "file_a = f\"{exec_folder}/network_lomap.dat\"\n",
    "# file_a = f\"a-optimal-lomap.csv\"\n",
    "file_b = f\"{exec_folder}/network_rbfenn.dat\"\n",
    "# file_b = f\"d-optimal-lomap.csv\"\n",
    "\n",
    "perts_rbfenn, ligs = pipeline.analysis.get_info_network(file_a)\n",
    "perts_lomap, ligs = pipeline.analysis.get_info_network(file_b)\n",
    "perts = perts_rbfenn + perts_lomap\n",
    "\n",
    "# write a file that contains the combined perts, directions are distinct\n",
    "combined_perts = []\n",
    "filtered_out = 0\n",
    "for pert in perts:\n",
    "    if not pert in combined_perts:\n",
    "        combined_perts.append(pert)\n",
    "    else:\n",
    "        filtered_out += 1\n",
    "print(\n",
    "    f\"Removed {filtered_out} duplicate perts between lomap and rbfenn to give {len(combined_perts)} combined perts.\"\n",
    ")\n",
    "\n",
    "# write a file that contains the unique perts, 1 direction only.\n",
    "filtered_perts = []\n",
    "filtered_out = 0\n",
    "for pert in combined_perts:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if not pert in filtered_perts and not inv_pert in filtered_perts:\n",
    "        filtered_perts.append(pert)\n",
    "    else:\n",
    "        filtered_out += 1\n",
    "print(\n",
    "    f\"Removed {filtered_out} inverse perts to give {len(filtered_perts)} unique perts, one direction only.\"\n",
    ")\n",
    "\n",
    "# get the perts that are unique to each\n",
    "unique_perts = []\n",
    "unique_out_lomap = 0\n",
    "unique_out_rbfenn = 0\n",
    "shared_out = 0\n",
    "\n",
    "for pert in perts_lomap:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if not pert in perts_rbfenn and not inv_pert in perts_rbfenn:\n",
    "        unique_perts.append((pert, \"lomap\"))\n",
    "        unique_out_lomap += 1\n",
    "\n",
    "for pert in perts_rbfenn:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if not pert in perts_lomap and not inv_pert in perts_lomap:\n",
    "        unique_perts.append((pert, \"rbfenn\"))\n",
    "        unique_out_rbfenn += 1\n",
    "\n",
    "for pert in combined_perts:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if pert in perts_lomap or inv_pert in perts_lomap:\n",
    "        if pert in perts_rbfenn or inv_pert in perts_rbfenn:\n",
    "            unique_perts.append((pert, \"shared\"))\n",
    "            shared_out += 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"There are {unique_out_lomap} pert(s) unique to lomap and {unique_out_rbfenn} pert(s) unique to rbfenn.\"\n",
    ")\n",
    "print(f\"There are {shared_out} pert(s) shared between lomap and rbfenn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually combined all perts,\n",
    "perts_all, ligs = pipeline.analysis.get_info_network(\"tyk2_all_perts.dat\")\n",
    "print(len(perts_all))\n",
    "# print(perts_all)\n",
    "\n",
    "all_perts = []\n",
    "\n",
    "for pert in perts_all:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if pert in combined_perts or inv_pert in combined_perts:\n",
    "        all_perts.append((pert, \"old\"))\n",
    "    else:\n",
    "        all_perts.append((pert, \"new\"))\n",
    "\n",
    "df = pd.DataFrame(all_perts)\n",
    "\n",
    "df.loc[df[1] == \"new\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{exec_folder}/combined_perts.dat\", \"w\") as writefile:\n",
    "    writer = csv.writer(writefile)\n",
    "    for pert in combined_perts:\n",
    "        writer.writerow([pert])\n",
    "print(f\"Total number of combined perturbations: {len(combined_perts)}\")\n",
    "\n",
    "with open(f\"{exec_folder}/filtered_perts.dat\", \"w\") as writefile:\n",
    "    writer = csv.writer(writefile)\n",
    "    for pert in filtered_perts:\n",
    "        writer.writerow([pert])\n",
    "print(f\"Total number of filtered perturbations: {len(filtered_perts)}\")\n",
    "\n",
    "# write a file for the different perts\n",
    "with open(f\"{exec_folder}/unique_perts.dat\", \"w\") as writefile:\n",
    "    writer = csv.writer(writefile)\n",
    "    for pert in unique_perts:\n",
    "        writer.writerow([pert[0], pert[1]])\n",
    "print(\n",
    "    f\"Total number of unique perturbations: {len(unique_perts)} (lomap: {unique_out_lomap}, rbfenn: {unique_out_rbfenn})\"\n",
    ")\n",
    "print(f\"Total number of shared perturbations: {shared_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dicitonary of the perts for plotting the nx graph\n",
    "both_pert_networks_dict = {}\n",
    "\n",
    "for pert in filtered_perts:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if pert in perts_lomap and pert in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"both\"\n",
    "    elif inv_pert in perts_lomap and pert in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"both\"\n",
    "    elif pert in perts_lomap and pert not in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"lomap\"\n",
    "    elif inv_pert in perts_lomap and pert not in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"lomap\"\n",
    "    elif pert not in perts_lomap and pert in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"rbfenn\"\n",
    "    elif inv_pert not in perts_lomap and pert in perts_rbfenn:\n",
    "        both_pert_networks_dict[pert] = \"rbfenn\"\n",
    "\n",
    "# create dict for images for the nx graph\n",
    "image_dict = {}\n",
    "# list files in inputs\n",
    "input_files_for_image = sorted(\n",
    "    os.listdir(f\"{exec_folder}/visualise_network_lomap/inputs\")\n",
    ")\n",
    "for in_file in input_files_for_image:\n",
    "    lig_name_list = in_file.split(\"_\")[1:]\n",
    "    lig_name = \"_\".join(lig_name_list).split(\".\")[0]\n",
    "    lig_number = in_file.split(\"_\")[0]\n",
    "    image_dict[lig_name] = lig_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_pert_networks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph.\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Loop over the nligands and add as nodes to the graph.\n",
    "for lig in ligand_names:\n",
    "    img = f\"{exec_folder}/visualise_network_lomap/images/{image_dict[lig]}.png\"\n",
    "    graph.add_node(lig, image=img, label=lig, labelloc=\"t\")\n",
    "\n",
    "# make a dict of colours\n",
    "# navy teal  #CC00CC\n",
    "# clear is '#FF000000'\n",
    "colour_dict = {\"both\": \"navy\", \"lomap\": \"teal\", \"rbfenn\": \"hotpink\"}\n",
    "\n",
    "# Loop over the edges in the dictionary and add to the graph.\n",
    "for edge in both_pert_networks_dict:\n",
    "    graph.add_edge(\n",
    "        edge.split(\"~\")[0],\n",
    "        edge.split(\"~\")[1],\n",
    "        color=colour_dict[both_pert_networks_dict[edge]],\n",
    "    )\n",
    "\n",
    "# Plot the networkX graph.\n",
    "pos = nx.kamada_kawai_layout(graph)\n",
    "colours = nx.get_edge_attributes(graph, \"color\").values()\n",
    "\n",
    "plt.figure(figsize=(12, 12), dpi=150)\n",
    "nx.draw(\n",
    "    graph,\n",
    "    pos,\n",
    "    edge_color=colours,\n",
    "    width=1,\n",
    "    linewidths=5,\n",
    "    node_size=2000,\n",
    "    node_color=\"skyblue\",\n",
    "    font_size=12,\n",
    "    labels={node: node for node in graph.nodes()},\n",
    ")\n",
    "\n",
    "plt.savefig(f\"{exec_folder}/compared_networks_no_images.png\", dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# Convert to a dot graph.\n",
    "dot_graph = nx.drawing.nx_pydot.to_pydot(graph)\n",
    "\n",
    "# Write to a PNG.\n",
    "network_plot = f\"{exec_folder}/compared_networks.png\"\n",
    "dot_graph.write_png(network_plot)\n",
    "\n",
    "# Create a plot of the network.\n",
    "img = mpimg.imread(network_plot)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the lomap score for the combined network file (unfiltered)\n",
    "combined_pert_network_dict = {}\n",
    "\n",
    "for pert in combined_perts:\n",
    "    lig_a = pert.split(\"~\")[0]\n",
    "    lig_b = pert.split(\"~\")[1]\n",
    "    # then, we need to find this index for our chosen edge that we are adding.\n",
    "    lig_a_index = ligand_names.index(lig_a)\n",
    "    lig_b_index = ligand_names.index(lig_b)\n",
    "    # finally, we need to calculate this single lomap score.\n",
    "    single_transformation, single_lomap_score = BSS.Align.generateNetwork(\n",
    "        [ligands[lig_a_index], ligands[lig_b_index]],\n",
    "        names=[ligand_names[lig_a_index], ligand_names[lig_b_index]],\n",
    "        plot_network=False,\n",
    "    )\n",
    "    print(f\"LOMAP score for {lig_a} to {lig_b} is {single_lomap_score[0]} .\")\n",
    "    combined_pert_network_dict[(lig_a, lig_b)] = single_lomap_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the combined to a different network file\n",
    "\n",
    "with open(f\"{exec_folder}/network_combined.dat\", \"w\") as network_file:\n",
    "    writer = csv.writer(network_file, delimiter=\" \")\n",
    "\n",
    "    for pert, score in combined_pert_network_dict.items():\n",
    "        # # based on the provided (at top of notebook) lambda allocations and LOMAP threshold, decide allocation.\n",
    "        # if score == None or score < float(node.getInput(\"LOMAP Threshold\")):\n",
    "        #     num_lambda = node.getInput(\"DiffLambdaWindows\")\n",
    "        # else:\n",
    "        #     num_lambda = node.getInput(\"LambdaWindows\")\n",
    "\n",
    "        num_lambda = node.getInput(\"LambdaWindows\")\n",
    "\n",
    "        # given the number of allocated lambda windows, generate an array for parsing downstream.\n",
    "        lam_array_np = np.around(np.linspace(0, 1, int(num_lambda)), decimals=5)\n",
    "\n",
    "        # make the array into a format readable by bash.\n",
    "        lam_array = (\n",
    "            str(lam_array_np)\n",
    "            .replace(\"[ \", \"\")\n",
    "            .replace(\"]\", \"\")\n",
    "            .replace(\"  \", \",\")\n",
    "            .replace(\"\\n\", \"\")\n",
    "        )\n",
    "\n",
    "        # write out both directions for this perturbation.\n",
    "        if engine == \"ALL\":\n",
    "            for eng in BSS.FreeEnergy.engines():\n",
    "                writer.writerow([pert[0], pert[1], len(lam_array_np), lam_array, eng])\n",
    "        else:\n",
    "            writer.writerow([pert[0], pert[1], len(lam_array_np), lam_array, engine])\n",
    "        # writer.writerow([pert[1], pert[0], len(lam_array_np), lam_array, engine])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate a and d optimal networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code in other_workflows.\n",
    "this has 4 files\n",
    "exp_dg - this is just experimental dGs\n",
    "pairwise_fep - this is the results from the fep (A, D, lit)\n",
    "literature designed pairs - this is the pairs from literature and their fep ddG\n",
    "ap_matrix - this is the matrix of the ligands with their similarity.\n",
    "\n",
    "scripts are in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = \"p38\"\n",
    "\n",
    "exec_folder = f\"/home/anna/Documents/benchmark/{protein}_benchmark/execution_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate a network, need to make matrix as for cdk2\n",
    "\n",
    "links_file = f\"{exec_folder}/links_file.in\"\n",
    "\n",
    "# rbfenn file\n",
    "\n",
    "value_dict = {}\n",
    "\n",
    "# need to normalise the rbfenn so less close to 0 as otherwise is not able to det(matrix) way more than 0 when using the R script.\n",
    "\n",
    "with open(links_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        lig0 = line.split(\" \")[0].strip()\n",
    "        lig1 = line.split(\" \")[1].strip()\n",
    "        score = line.split(\" \")[2].strip()\n",
    "        if lig0 not in value_dict.keys():\n",
    "            value_dict[lig0] = {}\n",
    "            value_dict[lig0][lig1] = score\n",
    "        else:\n",
    "            value_dict[lig0][lig1] = score\n",
    "\n",
    "pd.options.display.float_format = \"{:,.5f}\".format\n",
    "df = pd.DataFrame(value_dict).fillna(1)\n",
    "\n",
    "df = df.sort_index()\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "df.to_csv(\n",
    "    f\"{exec_folder}/{protein}_rbfenn_matrix.csv\",\n",
    "    index_label=\"ID\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lomap file\n",
    "\n",
    "value_dict = {}\n",
    "\n",
    "# initialise pipeline at top of file\n",
    "pl = initialise_pipeline()\n",
    "# where the ligands for the pipeline are located. These should all be in the same folder in sdf format\n",
    "pl.ligands_folder(f\"/home/anna/Documents/benchmark/inputs/{protein}/ligands\")\n",
    "# where the pipeline should be made\n",
    "pl.main_folder(\"/home/anna/Documents/code/write/test\")\n",
    "pl.setup_ligands()\n",
    "\n",
    "for lig0, lig1 in it.product(pl.ligand_names, pl.ligand_names):\n",
    "    if lig0 != lig1:\n",
    "        single_transformation, score = BSS.Align.generateNetwork(\n",
    "            [pl.ligands_dict[lig0], pl.ligands_dict[lig1]],\n",
    "            names=[lig0, lig1],\n",
    "            plot_network=False,\n",
    "            links_file=None,\n",
    "        )\n",
    "        if lig0 not in value_dict.keys():\n",
    "            value_dict[lig0] = {}\n",
    "            value_dict[lig0][lig1] = score[0]\n",
    "        else:\n",
    "            value_dict[lig0][lig1] = score[0]\n",
    "\n",
    "df = pd.DataFrame(value_dict).fillna(1)\n",
    "\n",
    "df.to_csv(f\"{exec_folder}/{protein}_lomap_matrix.csv\", index_label=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expected performance of lomap vs rbfenn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run in python script already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = \"mcl1\"\n",
    "\n",
    "exec_folder = f\"/home/anna/Documents/benchmark/{protein}_benchmark/execution_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make best fit dict\n",
    "best_fit_dict = {}\n",
    "\n",
    "for name in [\n",
    "    \"lomap\",\n",
    "    \"rbfenn\",\n",
    "    \"lomap-a-optimal\",\n",
    "    \"rbfenn-a-optimal\",\n",
    "    \"lomap-d-optimal\",\n",
    "    \"rbfenn-d-optimal\",\n",
    "]:\n",
    "    df = pd.read_csv(f\"{exec_folder}/network_{name}_stats_20.csv\", header=None)\n",
    "\n",
    "    x = np.array(df[0])\n",
    "    no_bins = abs(round(math.sqrt(len(x))))\n",
    "\n",
    "    # Fit a normal distribution to the data, mean and standard deviation\n",
    "    mu, std = norm.fit(x)\n",
    "\n",
    "    # plot histogram\n",
    "    plt.hist(\n",
    "        x, bins=no_bins, density=True, alpha=0.7, color=\"skyblue\", edgecolor=\"grey\"\n",
    "    )\n",
    "\n",
    "    # Plot the PDF.\n",
    "    xmin, xmax = plt.xlim()\n",
    "\n",
    "    mu, std = norm.fit(x)\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    y = norm.pdf(x, mu, std)\n",
    "\n",
    "    best_fit_dict[name] = ((x, y), mu, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dict = {}\n",
    "\n",
    "for key in best_fit_dict.keys():\n",
    "    hexadecimal_alphabets = \"0123456789ABCDEF\"\n",
    "    color = \"#\" + \"\".join([random.choice(hexadecimal_alphabets) for j in range(6)])\n",
    "    col_dict[key] = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the differnet neworks\n",
    "\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "lines = []\n",
    "mu_std_string = \"\"\n",
    "\n",
    "for name in best_fit_dict.keys():\n",
    "    col = col_dict[name]\n",
    "    plt.plot(\n",
    "        best_fit_dict[name][0][0],\n",
    "        best_fit_dict[name][0][1],\n",
    "        \"k\",\n",
    "        linewidth=2,\n",
    "        color=col,\n",
    "    )\n",
    "    lines += plt.plot(0, 0, c=col, label=name)\n",
    "    mu_std_string += f\"\\n{name} : mu = {best_fit_dict[name][1]:.3f} , std = {best_fit_dict[name][2]:.3f}\"\n",
    "\n",
    "    ax.axvspan(\n",
    "        best_fit_dict[name][1] + best_fit_dict[name][2],\n",
    "        best_fit_dict[name][1] - best_fit_dict[name][2],\n",
    "        color=col,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "labels = [l.get_label() for l in lines]\n",
    "plt.legend(lines, labels, loc=\"upper right\")\n",
    "\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"{mu_std_string}\")\n",
    "plt.savefig(\n",
    "    f\"dist_errors_networks.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biosimspace-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d79bb85316fa6c998e385cc39903e056bffeb3f6098416e9c269ddd32175e919"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
